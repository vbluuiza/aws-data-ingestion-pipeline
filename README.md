# ğŸ“¦ Projeto: Pipeline de IngestÃ£o de Dados na AWS (Camada Bronze)

Este Ã© um projeto **acadÃªmico** com o objetivo de construir uma pipeline de ingestÃ£o de dados utilizando **AWS S3**, estruturando a **camada Bronze** de um Data Lake.

---

## ğŸ¯ Objetivos do Projeto

Este projeto foi desenvolvido com o propÃ³sito de aplicar conceitos de Engenharia de Dados utilizando os principais serviÃ§os da AWS. AtravÃ©s de um fluxo simples e modular, busca-se representar a primeira etapa de um Data Lake.

- Construir uma pipeline de ingestÃ£o de dados baseada em arquivos CSV pÃºblicos
- Utilizar o Amazon S3 como destino da camada bronze
- Estruturar o Data Lake com o uso do AWS Lake Formation
- Modularizar o processo de extraÃ§Ã£o, transformaÃ§Ã£o e upload com boas prÃ¡ticas de cÃ³digo
- Consolidar o aprendizado sobre seguranÃ§a e controle de acesso a dados na nuvem

---

## âœ… Funcionalidades implementadas

- `extract_data.py`: faz o download de arquivos CSV pÃºblicos e salva localmente.
- `load_dfs_by_year.py`: carrega e organiza arquivos `.csv` por ano em DataFrames.
- `upload_parquet.py`: converte DataFrames em Parquet (em memÃ³ria) e envia para o S3.
- `setup_path.py`: adiciona o diretÃ³rio raiz ao `sys.path` para facilitar os imports.
- Notebooks demonstrativos com a lÃ³gica de extraÃ§Ã£o, transformaÃ§Ã£o e upload.

---

## ğŸ“ Estrutura de DiretÃ³rios

```bash
pipeline-aws-data-lake/
â”œâ”€â”€ aws/                    # SessÃ£o AWS e credenciais
â”œâ”€â”€ config/                 # Setup de path e arquivos de configuraÃ§Ã£o
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                # Arquivos .csv baixados
â”œâ”€â”€ notebooks/              # Notebooks de demonstraÃ§Ã£o da lÃ³gica
â”œâ”€â”€ s3_upload/              # Upload para S3 (Parquet)
â”œâ”€â”€ scripts/                # Scripts de extraÃ§Ã£o e pipeline
â”œâ”€â”€ transform/              # TransformaÃ§Ã£o dos dados
â”œâ”€â”€ .env                    # VariÃ¡veis de ambiente (chaves AWS, bucket, etc.)
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â””â”€â”€ README.md

```

---

## â˜ï¸ ServiÃ§os AWS utilizados

â˜ï¸ **ServiÃ§os AWS utilizados**

- âœ… Amazon S3 (armazenamento dos dados na camada bronze)
- âœ… AWS Lake Formation (estruturaÃ§Ã£o do Data Lake e controle de acesso)

---

## ğŸ› ï¸ Stack utilizada

- `Python`
- `boto3`
- `pandas`
- `pyarrow`
- `python-dotenv`
- `requests`
- `os`, `pathlib.Path`, `io.BytesIO`

---

## ğŸ“” Notebooks

Os notebooks presentes no diretÃ³rio `notebooks/` tÃªm propÃ³sito **demonstrativo**, explicando a lÃ³gica dos scripts principais e realizando alguns testes isolados. A execuÃ§Ã£o oficial serÃ¡ realizada via scripts Python.

---

ğŸš§ Este projeto ainda estÃ¡ em andamento e serÃ¡ atualizado conforme evolui com novas etapas e integraÃ§Ãµes Ã  arquitetura de dados em nuvem.

---

ğŸ‘©â€ğŸ’» Desenvolvedora: [Luiza Vieira](https://www.linkedin.com/in/vbluuiza/)
